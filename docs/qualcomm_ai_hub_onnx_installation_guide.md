# Guide d'Installation Complet : Qualcomm AI Hub avec ONNX Runtime sur Windows Snapdragon X Plus

Ce guide d√©taille la proc√©dure compl√®te d'installation et de configuration n√©cessaire pour ex√©cuter des mod√®les depuis le Qualcomm AI Hub en utilisant ONNX Runtime avec optimisation NPU Hexagon sur Windows avec architecture Snapdragon X Plus.

## ‚ö†Ô∏è Contrainte Architecturale Critique

**POINT CRUCIAL :** Pour utiliser qai_hub_models et ONNX Runtime QNN sur Windows Snapdragon X Plus/Elite, vous **DEVEZ** utiliser Python x64 (AMD64) en mode √©mulation. Les packages Python ARM64 natifs ne sont **PAS support√©s** et causeront des √©checs d'installation.

```bash
# ‚ùå NE PAS UTILISER : Python ARM64 natif
# ‚úÖ UTILISER : Python x64 (AMD64) en √©mulation
```

## 1. Pr√©requis Syst√®me

### Configuration Hardware Requise
- **Processeur :** Snapdragon X Plus 8-Core ou X Elite
- **RAM :** Minimum 16GB (32GB recommand√© pour mod√®les larges)
- **Stockage :** 50GB d'espace libre minimum
- **OS :** Windows 11 version 22H2 ou sup√©rieure

### V√©rification du Syst√®me
```powershell
# V√©rifier l'architecture du processeur
Get-ComputerInfo | Select-Object CsProcessors

# V√©rifier la version Windows
winver

# V√©rifier la pr√©sence du NPU Hexagon
Get-PnpDevice -FriendlyName "*Hexagon*"
```

## 2. Installation Python x64 (OBLIGATOIRE)

### √âtape 1 : T√©l√©chargement Python x64
```bash
# T√©l√©charger Python 3.11.x AMD64 depuis python.org
# URL : https://www.python.org/downloads/windows/
# S√©lectionner imp√©rativement "Windows installer (64-bit)"
```

### √âtape 2 : V√©rification de l'Installation
```powershell
# V√©rifier que Python s'ex√©cute en x64
python -c "import platform; print(f'Architecture: {platform.architecture()}')"
# Sortie attendue : Architecture: ('64bit', 'WindowsPE')

python -c "import platform; print(f'Machine: {platform.machine()}')"
# Sortie attendue : Machine: AMD64
```

## 3. Configuration Environnement Virtuel

### Cr√©ation de l'Environnement
```bash
# Cr√©er un environnement virtuel d√©di√©
python -m venv qai_hub_env

# Activation (Windows)
qai_hub_env\Scripts\activate

# V√©rification de l'environnement
python -c "import sys; print(sys.executable)"
```

### Mise √† Jour des Outils de Base
```bash
# Mise √† jour pip, setuptools et wheel
python -m pip install --upgrade pip setuptools wheel

# Installer des outils de compilation (requis pour certaines d√©pendances)
pip install wheel setuptools-scm
```

## 4. Installation des D√©pendances Principales

### √âtape 1 : Installation ONNX Runtime QNN
```bash
# Installation ONNX Runtime avec support QNN Execution Provider
pip install onnxruntime-qnn

# OU pour la version nightly (plus r√©cente)
pip install --pre --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple onnxruntime-qnn
```

### √âtape 2 : Installation Qualcomm AI Hub
```bash
# Installation qai-hub (client pour services cloud)
pip install qai-hub

# Installation qai-hub-models (mod√®les pr√©-optimis√©s)
pip install qai_hub_models

# V√©rification des installations
python -c "import onnxruntime; print(f'ONNX Runtime version: {onnxruntime.__version__}')"
python -c "import qai_hub; print('QAI Hub install√© avec succ√®s')"
```

### √âtape 3 : D√©pendances Additionnelles
```bash
# Biblioth√®ques de traitement audio/image
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install numpy opencv-python pillow librosa

# Gestion audio pour Snapdragon X Plus
pip install pipwin
pipwin install pyaudio

# OU alternative si pipwin √©choue
pip install pyaudio
```

## 5. Configuration Qualcomm AI Hub

### √âtape 1 : Cr√©ation Compte D√©veloppeur
1. Aller sur [aihub.qualcomm.com](https://aihub.qualcomm.com)
2. Cr√©er un compte avec votre Qualcomm ID
3. Activer l'acc√®s d√©veloppeur

### √âtape 2 : Configuration Token API
```bash
# Obtenir le token API
# 1. Se connecter √† AI Hub
# 2. Aller dans Account -> Settings -> API Token
# 3. Copier le token

# Configuration du client
qai-hub configure --api_token VOTRE_TOKEN_API

# V√©rification de la configuration
qai-hub whoami
```

### √âtape 3 : Test de Connectivit√©
```bash
# Lister les dispositifs disponibles
qai-hub list-devices

# Rechercher le Snapdragon X Plus
qai-hub list-devices | grep -i "snapdragon x plus"
```

## 6. Installation et Configuration NPU Hexagon

### √âtape 1 : Installation Pilote NPU
1. **T√©l√©charger Qualcomm Package Manager 3** depuis developer.qualcomm.com
2. **Modifier OS Setting** : Changer de "Windows" vers "Windows (ARM64)"
3. **Navigation** : Neural processors > Snapdragon X Elite NPU
4. **Installer** pilote NPU version 1.0.0.10 minimum
5. **Red√©marrer** le syst√®me

### √âtape 2 : V√©rification NPU
```powershell
# V√©rifier dans le Gestionnaire de p√©riph√©riques
Get-PnpDevice -FriendlyName "*Qualcomm*Hexagon*NPU*"

# Tester la d√©tection via Python
python -c "import onnxruntime as ort; print('Providers disponibles:', ort.get_available_providers())"
# Vous devriez voir 'QNNExecutionProvider' dans la liste
```

## 7. Configuration ONNX Runtime QNN

### √âtape 1 : Variables d'Environnement
```powershell
# Ajouter le chemin des DLL QNN (g√©n√©ralement inclus avec onnxruntime-qnn)
$env:PATH += ";$env:USERPROFILE\AppData\Local\Programs\Python\Python311\Lib\site-packages\onnxruntime\capi"
```

### √âtape 2 : Test Configuration QNN
```python
# test_qnn_setup.py
import onnxruntime as ort
import numpy as np

def test_qnn_availability():
    """Test la disponibilit√© du QNN Execution Provider"""
    
    # V√©rifier les providers disponibles
    providers = ort.get_available_providers()
    print(f"Providers disponibles: {providers}")
    
    if "QNNExecutionProvider" in providers:
        print("‚úÖ QNN Execution Provider d√©tect√©")
        return True
    else:
        print("‚ùå QNN Execution Provider non disponible")
        return False

def test_qnn_session():
    """Test cr√©ation session avec QNN"""
    try:
        # Options de session
        options = ort.SessionOptions()
        options.add_session_config_entry("session.disable_cpu_ep_fallback", "1")
        
        # Cr√©ation session (sans mod√®le pour ce test)
        print("Test de cr√©ation session QNN...")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation session QNN: {e}")
        return False

if __name__ == "__main__":
    print("=== Test Configuration ONNX Runtime QNN ===")
    qnn_available = test_qnn_availability()
    
    if qnn_available:
        test_qnn_session()
    else:
        print("\nüîß Solutions possibles:")
        print("1. R√©installer onnxruntime-qnn")
        print("2. V√©rifier l'installation du pilote NPU")
        print("3. Red√©marrer le syst√®me")
```

## 8. Test avec Mod√®le Whisper Base EN

### √âtape 1 : Import et Chargement
```python
# test_whisper_qualcomm.py
import qai_hub_models
from qai_hub_models.models.whisper_base_en import Model, App
import numpy as np
import torch

def test_whisper_model():
    """Test du mod√®le Whisper Base EN Qualcomm"""
    
    try:
        print("üì¶ Chargement du mod√®le Whisper Base EN...")
        model = Model.from_pretrained()
        app = App(model)
        print("‚úÖ Mod√®le charg√© avec succ√®s")
        
        # Test avec input factice
        print("üß™ Test avec input audio factice...")
        sample_inputs = model.sample_inputs()
        
        # Ex√©cution locale
        output = model(**sample_inputs)
        print("‚úÖ Inf√©rence locale r√©ussie")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur test mod√®le: {e}")
        return False

if __name__ == "__main__":
    test_whisper_model()
```

### √âtape 2 : Compilation pour Snapdragon X Plus
```python
# compile_whisper_for_snapdragon.py
import qai_hub as hub
import torch
from qai_hub_models.models.whisper_base_en import Model

def compile_whisper_for_snapdragon():
    """Compile le mod√®le Whisper pour Snapdragon X Plus"""
    
    try:
        # Chargement mod√®le
        print("üì¶ Chargement mod√®le Whisper...")
        torch_model = Model.from_pretrained()
        torch_model.eval()
        
        # Tra√ßage du mod√®le
        print("üîÑ Tra√ßage du mod√®le...")
        sample_inputs = torch_model.sample_inputs()
        input_specs = {}
        traced_inputs = []
        
        for name, (shape, dtype) in sample_inputs.items():
            input_specs[name] = shape
            traced_inputs.append(torch.randn(shape).to(dtype))
        
        traced_model = torch.jit.trace(torch_model, traced_inputs)
        
        # Compilation pour Snapdragon X Plus avec ONNX Runtime
        print("üöÄ Soumission job de compilation...")
        compile_job = hub.submit_compile_job(
            model=traced_model,
            device=hub.Device("Snapdragon X Plus 8-Core CRD"),
            input_specs=input_specs,
            options="--target_runtime onnx"
        )
        
        print(f"üìã Job ID: {compile_job.job_id}")
        print("‚è≥ Attente de la compilation...")
        
        # Attendre la compilation
        target_model = compile_job.get_target_model()
        print("‚úÖ Compilation termin√©e!")
        
        # Profiling sur dispositif r√©el
        print("üìä Lancement profiling...")
        profile_job = hub.submit_profile_job(
            model=target_model,
            device=hub.Device("Snapdragon X Plus 8-Core CRD")
        )
        
        # Attendre les r√©sultats
        profile_data = profile_job.download_profile()
        print("‚úÖ Profiling termin√©!")
        
        return target_model, profile_data
        
    except Exception as e:
        print(f"‚ùå Erreur compilation: {e}")
        return None, None

if __name__ == "__main__":
    model, profile = compile_whisper_for_snapdragon()
    if model:
        print(f"üéâ Mod√®le compil√©: {model}")
```

## 9. Ex√©cution Locale avec ONNX Runtime QNN

### Configuration Session Optimis√©e
```python
# local_inference_optimized.py
import onnxruntime as ort
import numpy as np
from pathlib import Path

def create_optimized_qnn_session(model_path):
    """Cr√©e une session ONNX Runtime optimis√©e pour QNN"""
    
    # Options de session
    session_options = ort.SessionOptions()
    
    # Optimisations performance
    session_options.inter_op_num_threads = 4
    session_options.intra_op_num_threads = 4
    session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    
    # Configuration QNN sp√©cifique
    session_options.add_session_config_entry("session.disable_cpu_ep_fallback", "1")
    session_options.add_session_config_entry("ep.context_enable", "1")
    
    # Provider options pour QNN
    qnn_provider_options = {
        "backend_path": "QnnHtp.dll",  # NPU Hexagon
        "profiling_level": "basic",
        "rpc_control_latency": "low",
        "vtcm_mb": "8"
    }
    
    try:
        # Cr√©ation session avec QNN EP
        session = ort.InferenceSession(
            str(model_path),
            sess_options=session_options,
            providers=["QNNExecutionProvider"],
            provider_options=[qnn_provider_options]
        )
        
        print("‚úÖ Session QNN cr√©√©e avec succ√®s")
        print(f"üìä Providers actifs: {session.get_providers()}")
        
        return session
        
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation session QNN: {e}")
        
        # Fallback vers CPU
        print("üîÑ Fallback vers CPU...")
        return ort.InferenceSession(str(model_path))

def run_inference_with_qnn(session, input_data):
    """Ex√©cute l'inf√©rence avec la session QNN"""
    
    try:
        # Pr√©paration des inputs
        input_name = session.get_inputs()[0].name
        inputs = {input_name: input_data}
        
        # Mesure de performance
        import time
        start_time = time.time()
        
        # Inf√©rence
        outputs = session.run(None, inputs)
        
        end_time = time.time()
        inference_time = (end_time - start_time) * 1000  # en ms
        
        print(f"‚ö° Temps d'inf√©rence: {inference_time:.2f}ms")
        
        return outputs
        
    except Exception as e:
        print(f"‚ùå Erreur inf√©rence: {e}")
        return None

# Exemple d'utilisation
if __name__ == "__main__":
    # Remplacer par le chemin vers votre mod√®le ONNX compil√©
    model_path = "whisper_base_en_snapdragon.onnx"
    
    if Path(model_path).exists():
        session = create_optimized_qnn_session(model_path)
        
        # Test avec donn√©es factices
        input_shape = (1, 80, 3000)  # Exemple pour Whisper
        test_input = np.random.randn(*input_shape).astype(np.float32)
        
        outputs = run_inference_with_qnn(session, test_input)
        print(f"üéØ Outputs shape: {[out.shape for out in outputs] if outputs else 'None'}")
```

## 10. D√©pannage Commun

### Probl√®me : QNNExecutionProvider Non Trouv√©
```bash
# Solutions :
1. V√©rifier l'installation : pip show onnxruntime-qnn
2. R√©installer : pip uninstall onnxruntime-qnn && pip install onnxruntime-qnn
3. V√©rifier le pilote NPU dans le Gestionnaire de p√©riph√©riques
4. Red√©marrer le syst√®me
```

### Probl√®me : Installation qai_hub_models √âchec
```bash
# V√©rifier l'architecture Python
python -c "import platform; print(platform.machine())"
# Doit afficher "AMD64", sinon r√©installer Python x64

# Solutions :
1. Utiliser Python x64 OBLIGATOIREMENT
2. Vider le cache pip : pip cache purge
3. Installer avec --no-cache-dir : pip install --no-cache-dir qai_hub_models
```

### Probl√®me : PyAudio Installation √âchec
```bash
# Solutions multiples :
pip install pipwin && pipwin install pyaudio
# OU
pip install --only-binary=all pyaudio
# OU compiler depuis les sources avec Visual Studio Build Tools
```

### Probl√®me : Mod√®le "Hang" sur NPU
```bash
# Solutions :
1. V√©rifier la quantification du mod√®le (FP16/INT8)
2. R√©duire la taille des inputs
3. Utiliser options QNN : "rpc_control_latency": "low"
4. Augmenter timeout : session_options.add_session_config_entry("timeout", "30000")
```

## 11. Scripts de Validation Compl√®te

### Script de Validation Totale
```python
# validate_complete_setup.py
import sys
import importlib
from pathlib import Path

def check_python_architecture():
    """V√©rifie l'architecture Python"""
    import platform
    arch = platform.machine()
    print(f"üîç Architecture Python: {arch}")
    return arch == "AMD64"

def check_required_packages():
    """V√©rifie les packages requis"""
    required_packages = [
        "onnxruntime",
        "qai_hub",
        "qai_hub_models",
        "torch",
        "numpy",
        "pyaudio"
    ]
    
    results = {}
    for package in required_packages:
        try:
            importlib.import_module(package.replace("-", "_"))
            results[package] = "‚úÖ OK"
        except ImportError:
            results[package] = "‚ùå MANQUANT"
    
    return results

def check_qnn_availability():
    """V√©rifie QNN Execution Provider"""
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        return "QNNExecutionProvider" in providers
    except:
        return False

def check_ai_hub_auth():
    """V√©rifie l'authentification AI Hub"""
    try:
        import subprocess
        result = subprocess.run(["qai-hub", "whoami"], 
                              capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False

def main():
    """Validation compl√®te du setup"""
    print("üöÄ Validation Setup Qualcomm AI Hub + ONNX Runtime")
    print("=" * 60)
    
    # 1. Architecture Python
    arch_ok = check_python_architecture()
    if not arch_ok:
        print("‚ùå ERREUR CRITIQUE: Utiliser Python x64 (AMD64)")
        return False
    
    # 2. Packages
    packages = check_required_packages()
    print("\nüì¶ Packages:")
    for pkg, status in packages.items():
        print(f"  {pkg}: {status}")
    
    # 3. QNN Provider
    qnn_ok = check_qnn_availability()
    print(f"\nüß† QNN Execution Provider: {'‚úÖ OK' if qnn_ok else '‚ùå MANQUANT'}")
    
    # 4. AI Hub Auth
    auth_ok = check_ai_hub_auth()
    print(f"üîê AI Hub Auth: {'‚úÖ OK' if auth_ok else '‚ùå NON CONFIGUR√â'}")
    
    # R√©sum√©
    all_ok = arch_ok and all("‚úÖ" in status for status in packages.values()) and qnn_ok and auth_ok
    
    print("\n" + "=" * 60)
    if all_ok:
        print("üéâ SETUP COMPLET - Pr√™t pour l'inf√©rence!")
    else:
        print("‚ö†Ô∏è  SETUP INCOMPLET - V√©rifier les √©l√©ments marqu√©s ‚ùå")
    
    return all_ok

if __name__ == "__main__":
    main()
```

## 12. Commandes de R√©sum√© Installation

```bash
# Installation compl√®te en une fois (copier-coller)
# ATTENTION : Utiliser dans un terminal x64

# 1. Environnement virtuel
python -m venv qai_hub_env
qai_hub_env\Scripts\activate

# 2. Mise √† jour outils
python -m pip install --upgrade pip setuptools wheel

# 3. Packages principaux
pip install onnxruntime-qnn qai-hub qai_hub_models

# 4. D√©pendances PyTorch et audio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install numpy opencv-python pillow librosa
pip install pipwin && pipwin install pyaudio

# 5. Configuration AI Hub
qai-hub configure --api_token VOTRE_TOKEN

# 6. Test
python -c "import onnxruntime as ort; print('QNN disponible:', 'QNNExecutionProvider' in ort.get_available_providers())"
```

Ce guide couvre l'ensemble de la proc√©dure d'installation n√©cessaire pour ex√©cuter des mod√®les Qualcomm AI Hub avec ONNX Runtime sur Windows Snapdragon X Plus. La contrainte principale est l'utilisation obligatoire de Python x64 en √©mulation pour la compatibilit√© avec l'√©cosyst√®me Qualcomm.